# AUTOGENERATED! DO NOT EDIT! File to edit: 00_core.ipynb (unless otherwise specified).

__all__ = ['explode_types', 'explode_lens', 'explode_shapes', 'explode_ranges', 'pexpt', 'pexpl', 'pexps', 'get_cudas',
           'receptive_fields', 'ImageNTuple', 'ImageTupleBlock', 'ConditionalGenerator', 'SiameseCritic', 'GenMetric',
           'CriticMetric', 'l1', 'ProgressImage', 'download_file_from_google_drive', 'save_response_content',
           'FID_WEIGHTS_URL', 'renorm_stats', 'get_tuple_files_by_stem', 'ParentsSplitter', 'FilterRelToPath',
           'CGANDataLoaders', 'basic_name', 'GatherLogs', 'RunNBatches']

# Cell
import gc
import requests
from fastcore.all import *
from fastai.basics import *
from fastai.data.all import *
from fastai.vision.all import *
from fastai.vision.gan import *
from fastai.callback.hook import *
import pandas as pd
import seaborn as sns

# Cell
def explode_types(o):
    '''Like fastcore explode_types, but only shows __name__ of type.'''
    if not is_listy(o): return type(o).__name__
    return {type(o).__name__: [explode_types(o_) for o_ in o]}

# Cell
def explode_lens(o):
    if is_listy(o):
        if all(is_listy(o_) for o_ in o):
            return [explode_lens(o_) for o_ in o]
        else: return len(o)

# Cell
def explode_shapes(o):
    if not is_listy(o): return tuple(bind(getattr, arg0, 'shape')(o))
    return [explode_shapes(o_) for o_ in o]

# Cell
def explode_ranges(o):
    if not is_listy(o): return (float(o.min()), float(o.max()))
    return [explode_ranges(o_) for o_ in o]

# Cell
def pexpt(o): print(explode_types(o))

# Cell
def pexpl(o): print(explode_lens(o))

# Cell
def pexps(o): print(explode_shapes(o))

# Cell
def get_cudas():
    '''Returns the number of tensors in cuda device.'''
    n = 0
    for o in gc.get_objects():
        o = maybe_attr(o, 'data')
        if torch.is_tensor(o):
            if o.is_cuda: n += 1
    return n

# Cell
def receptive_fields(model, nf, imsize, bs=64):
    '''returns the size of the receptive field for each feature output.'''
    # init parameters
    for p in model.named_parameters():
        if 'weight' in p[0]:
            n = p[1].shape[1] if len(p[1].shape)==4 else 1
            nn.init.constant_(p[1], 1./n)
        elif 'bias' in p[0]:
            nn.init.constant_(p[1], 0)
    x = dummy_eval(model, imsize).detach()
    outsz = x.shape[-2:]

    with torch.no_grad():
        rfs = []
        model.eval()
        model = model.cuda()
        t = torch.eye(imsize[0]**2).reshape(imsize[0]*imsize[1], 1, imsize[0], imsize[1])
        for i,batch in enumerate(chunked(t, bs)):
            new = torch.cat(batch, dim=0).unsqueeze(1)
            new = torch.cat([new for _ in range(nf)], dim=1).cuda()
            rfs.append(model(new).cpu())
    rfs = torch.cat(rfs, dim=0).squeeze()
    rfs = rfs.reshape(imsize[0], imsize[1], outsz[0], outsz[1])
    rfs = (rfs>0.99).sum(axis=(0,1)).float().sqrt()
    return rfs

# Cell
class ImageNTuple(fastuple):

    @classmethod
    def create(cls, fns): return cls(tuple(PILImage.create(f) for f in tuplify(fns)))

    def show(self, ctx=None, **kwargs):
        all_tensors = all([isinstance(t, Tensor) for t in self])
        same_shape = all([self[0].shape==t.shape for t in self[1:]])
        if not all_tensors or not same_shape: return ctx
        line = self[0].new_zeros(self[0].shape[0], self[0].shape[1], 10)
        imgs = sum(L(zip(self, [line]*len(self))).map(list),[])[:-1]
        return show_image(torch.cat(imgs, dim=2), ctx=ctx, **kwargs)

    def requires_grad_(self, value):
        for item in self: item.requires_grad_(value)
        return self

    @property
    def shape(self):
        all_tensors = all([isinstance(t, Tensor) for t in self])
        same_shape = all([self[0].shape==t.shape for t in self[1:]])
        if not all_tensors or not same_shape: raise AttributeError
        return self[0].shape
    #def detach(self):
    #    for item in self: item.detach()
    #    return self

# Cell
def ImageTupleBlock():
    '''Like fastai tutoria siemese transform, but uses ImageNTuple.'''
    return TransformBlock(type_tfms=ImageNTuple.create, batch_tfms=[IntToFloatTensor])

# Cell
class ConditionalGenerator(nn.Module):
    '''Wraper around a GAN generator that returns the generated image and the inp.'''
    def __init__(self, gen):
        super().__init__()
        self.gen = gen
    def forward(self, x):
        if is_listy(x):
            inp = torch.cat(x, axis=1)
        else:
            inp = x
        return ImageNTuple(x, TensorImage(self.gen(inp)))

# Cell
class SiameseCritic(Module):
    def __init__(self, critic): self.critic = critic
    def forward(self, x): return self.critic(torch.cat(x, dim=1))

# Cell
class GenMetric(AvgMetric):
    def accumulate(self, learn):
        if learn.model.gen_mode:
            bs = find_bs(learn.yb)
            self.total += to_detach(self.func(learn, learn.pred, *learn.yb))*bs
            self.count += bs

# Cell
class CriticMetric(AvgMetric):
    def accumulate(self, learn):
        if not learn.model.gen_mode:
            bs = find_bs(learn.yb)
            self.total += to_detach(self.func(learn, learn.pred, *learn.yb))*bs
            self.count += bs

# Cell
def _l1(learn, output, target): return nn.L1Loss()(output[-1], target[-1])
l1 = GenMetric(_l1)

# Cell
@patch
def export(self:GANLearner, fname='export.pkl', pickle_protocol=2):
    "Export the content of `self` without the items and the optimizer state for inference"
    if rank_distrib(): return # don't export if child proc
    self.gan_trainer.switch(gen_mode=True)
    self._end_cleanup()
    old_dbunch = self.dls
    self.dls = self.dls.new_empty()
    state = self.opt.state_dict() if self.opt is not None else None
    self.opt = None
    with warnings.catch_warnings():
        #To avoid the warning that come from PyTorch about model not being checked
        warnings.simplefilter("ignore")
        torch.save(self, self.path/fname, pickle_protocol=pickle_protocol)
    set_freeze_model(self.model, True)
    self.create_opt()
    if state is not None: self.opt.load_state_dict(state)
    self.dls = old_dbunch
    self.gan_trainer.switch(gen_mode=True)

# Cell
class ProgressImage(Callback):
    run_after = GANTrainer
    @delegates(show_image)
    def __init__(self, out_widget, save_img=False, folder='pred_imgs', conditional=False, **kwargs):
        self.out_widget = out_widget
        self.kwargs = kwargs
        self.save_img = save_img
        self.folder = folder
        self.conditional = conditional
        if self.conditional:
            self.title = 'Input-Real-Fake'
        else:
            self.title = 'Generated'
        Path(self.folder).mkdir(exist_ok=True)
        self.ax = None
    def before_batch(self):
        if self.gan_trainer.gen_mode and self.training:
            self.last_gen_target = self.learn.to_detach(self.learn.yb)#[0][-1]
    def after_train(self):
        "Show a sample image."
        if not hasattr(self.learn.gan_trainer, 'last_gen'): return
        b = self.learn.gan_trainer.last_gen
        gt = self.last_gen_target
        #gt, b = self.learn.dls.decode((gt, b))
        b = self.learn.dls.decode((b,))
        gt = self.learn.dls.decode(gt)
        gt, imt = batch_to_samples((gt, b), max_n=1)[0]
        gt, imt = gt[0][-1], imt[0]
        if self.conditional:
            imt = ToTensor()(ImageNTuple.create((*imt[:-1], gt, imt[-1])))
        self.out_widget.clear_output(wait=True)
        with self.out_widget:
            if self.ax: self.ax.clear()
            self.ax = imt.show(ax=self.ax, title=self.title, **self.kwargs)
            display(self.ax.figure)
        if self.save_img: self.ax.figure.savefig(self.path / f'{self.folder}/pred_epoch_{self.epoch}.png')
    def after_fit(self):
        if self.ax: plt.close(self.ax.figure)

# Cell
@typedispatch
def show_results(x:TensorImage, y:ImageNTuple, samples, outs, ctxs=None, max_n=6, nrows=None, ncols=2, figsize=None, **kwargs):
    max_n = min(x.shape[0], max_n)
    if max_n<ncols: ncols = max_n
    if figsize is None: figsize = (ncols*6, max_n//ncols * 3)
    if ctxs is None: ctxs = get_grid(min(x[0].shape[0], max_n), nrows=None, ncols=ncols, figsize=figsize)
    for i,ctx in enumerate(ctxs):
        title = 'Input-Real-Fake'
        ImageNTuple(x[i], y[1][i], outs[i][0][1]).show(ctx=ctx, title=title)

# Cell
@patch
def show_results(self:GANLearner, ds_idx=1, dl=None, max_n=9, shuffle=True, **kwargs):
    if dl is None: dl = self.dls[ds_idx].new(shuffle=shuffle)
    b = dl.one_batch()
    _,_,preds = self.get_preds(dl=[b], with_decoded=True)
    preds = (preds,)
    self.dls.show_results(b, preds, max_n=max_n, **kwargs)

# Cell
URLs.FACADES = 'http://efrosgans.eecs.berkeley.edu/pix2pix/datasets/facades.tar.gz'
URLs.FACADES_BASE = 'http://cmp.felk.cvut.cz/~tylecr1/facade/CMP_facade_DB_base.zip'
URLs.FACADES_EXTENDED = 'http://cmp.felk.cvut.cz/~tylecr1/facade/CMP_facade_DB_extended.zip'
URLs.CELEBA = '0B7EVK8r0v71pZjFTYXZWM3FlRnM'

# Cell
def download_file_from_google_drive(file_id, destination, folder_name=None):
    if folder_name:
        dst = Config()['data'] / folder_name
        if dst.exists():
            return dst
    else:
        dst = Config()['data']
    arch_dst = Config()['archive'] / destination
    if not arch_dst.exists():
        URL = "https://docs.google.com/uc?export=download"
        session = requests.Session()
        response = session.get(URL, params = { 'id' : file_id }, stream = True)
        token = first([(k,v) for k,v in response.cookies.items() if k.startswith('download_warning')])[1]
        if token:
            params = { 'id' : file_id, 'confirm' : token }
            response = session.get(URL, params = params, stream = True)
        save_response_content(response, Config()['archive'] / destination)
    file_extract(Config()['archive'] / destination, Config()['data'])
    return dst

def save_response_content(response, destination):
    CHUNK_SIZE = 32768

    with open(destination, "wb") as f:
        for chunk in response.iter_content(CHUNK_SIZE):
            if chunk: # filter out keep-alive new chunks
                f.write(chunk)

# Cell
FID_WEIGHTS_URL = 'https://github.com/mseitzer/pytorch-fid/releases/download/fid_weights/pt_inception-2015-12-05-6726825d.pth'

# Cell
renorm_stats = (2*torch.tensor(imagenet_stats[0])-1).tolist(), (2*torch.tensor(imagenet_stats[1])).tolist()

# Cell
@patch
def is_relative_to(self:Path, *other):
    """Return True if the path is relative to another path or False.
    """
    try:
        self.relative_to(*other)
        return True
    except ValueError:
        return False

# Cell
def _parent_idxs(items, name):
    def _inner(items, name): return mask2idxs(Path(o).parent.name == name for o in items)
    return [i for n in L(name) for i in _inner(items,n)]

# Cell
@delegates(get_image_files)
def get_tuple_files_by_stem(paths, folders=None, **kwargs):
    if not is_listy(paths): paths = [paths]
    files = []
    for path in paths: files.extend(get_image_files(path, folders=folders))
    out = L(groupby(files, attrgetter('stem')).values())
    return out

# Cell
def ParentsSplitter(train_name='train', valid_name='valid'):
    "Split `items` from the grand parent folder names (`train_name` and `valid_name`)."
    def _inner(o):
        tindex = _parent_idxs(L(o).itemgot(-1), train_name)
        vindex = _parent_idxs(L(o).itemgot(-1), valid_name)
        return tindex, vindex
    return _inner

# Cell
class FilterRelToPath:
    def __init__(self, path): self.path = path
    def __call__(self, o): return L(o).filter(Self.is_relative_to(self.path))

# Cell
class CGANDataLoaders(DataLoaders):
    "Basic wrapper around several `DataLoader`s with factory methods for CGAN problems"
    @classmethod
    @delegates(DataLoaders.from_dblock)
    def from_paths(cls, input_path, target_path, train='train', valid='valid', valid_pct=None, seed=None, vocab=None, item_tfms=None,
                  batch_tfms=None, n_inp=1, add_normalize=True, **kwargs):
        "Create from imagenet style dataset in `path` with `train` and `valid` subfolders (or provide `valid_pct`)"
        splitter = ParentsSplitter(train_name=train, valid_name=valid) if valid_pct is None else RandomSplitter(valid_pct, seed=seed)
        get_items = get_tuple_files_by_stem if valid_pct else partial(get_tuple_files_by_stem, folders=[train, valid])
        if not Path(input_path).is_absolute(): input_path = Config()['data'] / input_path
        if not Path(target_path).is_absolute(): target_path = Config()['data'] / target_path
        if add_normalize:
            if batch_tfms is None:
                batch_tfms = [Normalize.from_stats([0.5]*3, [0.5]*3)]
            else:
                batch_tfms += [Normalize.from_stats([0.5]*3, [0.5]*3)]

        dblock = DataBlock(blocks=(ImageTupleBlock, ImageTupleBlock),
                           get_items=get_items,
                           splitter=splitter,
                           get_x=FilterRelToPath(input_path),
                           get_y=noop,
                           item_tfms=item_tfms,
                           batch_tfms=batch_tfms)
        return cls.from_dblock(dblock, [input_path, target_path], **kwargs)

    @classmethod
    @delegates(DataLoaders.from_dblock)
    def from_path_ext(cls, path, folders, input_ext='.png', output_ext='.jpg', valid_pct=0.2, seed=None, item_tfms=None,
                      batch_tfms=None, **kwargs):
        "Create from list of `fnames` in `path`s with `label_func`"
        get_itmes = partial(get_tuple_files_by_stem, folders=folders)
        files = get_itmes(path)
        dblock = DataBlock(blocks=(ImageBlock, ImageTupleBlock),
                           get_items=get_itmes,
                           splitter=RandomSplitter(valid_pct, seed=seed),
                           get_x=lambda o: L(o).filter(lambda x: x.suffix==input_ext)[0],
                           get_y=lambda o: L(o).sorted(key=lambda x: {input_ext:0, output_ext:1}[x.suffix]),
                           item_tfms=item_tfms,
                           batch_tfms=batch_tfms)
        return cls.from_dblock(dblock, path, **kwargs)

# Cell
def basic_name(flds=None):
    if isinstance(flds, str): flds = re.split(', *', flds)
    flds = list(flds or [])
    def _f(self):
        return '_'.join(f'{maybe_attr(nested_attr(self,o), "__name__")}' for o in flds)
    return _f

# Cell
class GatherLogs(Callback):
    run_after=Recorder
    def __init__(self, experiments='logs', save_after_fit=True):
        self.experiments = experiments
        self.file = Path(self.experiments + '.pkl')
        self.save_after_fit = save_after_fit
        self.experiment = None
        self.df = None
        self.experiment_count = defaultdict(int)

    def before_fit(self):
        self.values = L()
        if not self.experiment: self.set_experiment_name()

    def after_epoch(self): self.values.append(self.recorder.log)

    def set_experiment_name(self, name=None):
        self.experiment = ifnone(name, basic_name('model.__class__,loss_func.__class__')(self))
        self.experiment_count[name] += 1

    def after_fit(self):
        df = pd.DataFrame(self.values, columns=self.recorder.metric_names)
        df['experiment'] = self.experiment
        df['experiment_count'] = self.experiment_count[self.experiment]
        df['time'] = pd.to_timedelta(df['time'].map(lambda x: '00:'+x if x.count(':')==1 else x))
        df['time'] = df['time'].map(methodcaller('total_seconds'))
        df = self.to_tidy(df)
        self.df = pd.concat([self.df, df]).reset_index(drop=True)
        if self.save_after_fit: self.save()

    def to_tidy(self, df):
        '''Creates tidy dataframe from metrics dataframe.'''
        df = df.set_index(['epoch', 'time', 'experiment', 'experiment_count'])
        df = df.stack().reset_index(level=4)
        df['stage'] = df['level_4'].str.split('_').map(itemgetter(0))
        df['metric'] = df['level_4'].str.split('_', n=1).map(itemgetter(1))
        df = df.drop('level_4', axis=1)
        df = df.set_index(['stage', 'metric'], append=True)
        df[0] = df[0].astype(float)
        df = df.unstack()
        df.columns = df.columns.get_level_values(1)
        df.columns.name = None
        return df.reset_index()

    @delegates(sns.relplot)
    def plot_metric(self, y='loss', x='epoch' , col='stage', hue='experiment', kind='line', **kwargs):
        return sns.relplot(data=self.df, x=x, y=y, col=col, hue=hue, kind=kind)
    @delegates(sns.barplot)
    def plot_time(self, x='experiment', y='time', **kwargs):
        return sns.barplot(data=self.df[self.df['stage']=='train'], x=x, y=y, **kwargs)

    def save(self):
        self.learn = None
        with self.file.open('bw') as f: pickle.dump(self, f)

# Cell
class RunNBatches(Callback):
    run_after=Recorder
    def __init__(self, n=2, no_valid=True): store_attr()
    def before_train(self): self.n_batch = 0
    def before_validate(self):
        self.n_batch = 0
        if self.no_valid: raise CancelValidException
    def after_batch(self):
        self.n_batch += 1
        if self.n_batch>self.n:
            if self.training:
                raise CancelTrainException
            else:
                raise CancelValidException
    def after_cancel_train(self): self.recorder.cancel_train = False
    def after_cancel_validate(self): self.recorder.cancel_valid = False