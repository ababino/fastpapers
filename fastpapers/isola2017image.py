# AUTOGENERATED! DO NOT EDIT! File to edit: 01_isola2017image.ipynb (unless otherwise specified).

__all__ = ['GeneratorLoss', 'GeneratorBCE', 'gen_bce_l1_loss', 'gen_bce_loss', 'crit_bce_loss', 'crit_real_bce',
           'crit_fake_bce', 'Patch70', 'UnetUpsample', 'CGenerator', 'pix2pix_learner']

# Cell
from fastai.data.external import untar_data
from fastai.data.transforms import get_image_files
from fastai.data import *
from fastai.basics import *
from fastai.vision.all import *
from fastcore.all import *
from fastai.vision.gan import *
from fastai.callback.hook import *
from fastai.vision.widgets import *
from fastprogress import progress_bar, master_bar
from .core import *
import seaborn as sns

# Cell
class GeneratorLoss(nn.Module):
    def __init__(self, adversarial_loss_func, image_loss_func, adversarial_w, image_w):
        super().__init__()
        store_attr()
    def forward(self, fake_pred, output, target):
        adversarial_loss = self.adversarial_loss_func(fake_pred)
        image_loss = self.image_loss_func(output[-1], target[-1])
        return adversarial_loss*self.adversarial_w + image_loss*self.image_w

# Cell
class GeneratorBCE(nn.Module):
    def __init__(self):
        super().__init__()
        self.bce = nn.BCEWithLogitsLoss()
    def forward(self, fake_pred):
        #print(fake_pred.shape, fake_pred.dtype, type(fake_pred))
        if not hasattr(self, 'ones'): self.ones = fake_pred.new_ones(fake_pred.shape)
        if self.ones.shape!=fake_pred.shape: self.ones = fake_pred.new_ones(fake_pred.shape)
        return self.bce(fake_pred, self.ones)

# Cell
gen_bce_l1_loss = GeneratorLoss(GeneratorBCE(), nn.L1Loss(), 1, 100)

# Cell
def gen_bce_loss(learn, output, target):
    fake_pred = learn.model.critic(output)
    ones = fake_pred.new_ones(fake_pred.shape)
    bce = nn.BCEWithLogitsLoss()(fake_pred, ones)
    return bce

# Cell
def crit_bce_loss(real_pred, fake_pred):
    ones  = real_pred.new_ones(real_pred.shape)
    zeros = fake_pred.new_zeros(fake_pred.shape)
    loss_neg = nn.BCEWithLogitsLoss()(fake_pred, zeros)
    loss_pos = nn.BCEWithLogitsLoss()(real_pred, ones)
    return (loss_neg + loss_pos)/2

def crit_real_bce(learn, real_pred, input):
    ones  = real_pred.new_ones(real_pred.shape)
    rbce = nn.BCEWithLogitsLoss()(real_pred, ones)
    return rbce

def crit_fake_bce(learn, real_pred, input):
    fake = learn.model.generator(input).requires_grad_(False)
    fake_pred = learn.model.critic(fake)
    zeros = fake_pred.new_zeros(fake_pred.shape)
    fbce = nn.BCEWithLogitsLoss()(fake_pred, zeros)
    return fbce
#TypeError: new_ones(): argument 'dtype' must be torch.dtype, not int

# Cell
def Patch70(n_channels):
    layers = []
    layers.append(ConvLayer(n_channels, 64, ks=4, stride=2, norm_type=None, bias=False,
                           act_cls=partial(nn.LeakyReLU, negative_slope=0.2, inplace=True)))
    layers.append(ConvLayer(64, 128, ks=4, stride=2, norm_type=NormType.Batch, bias=False,
                           act_cls=partial(nn.LeakyReLU, negative_slope=0.2, inplace=True)))
    layers.append(ConvLayer(128, 256, ks=4, stride=2, norm_type=NormType.Batch, bias=False,
                           act_cls=partial(nn.LeakyReLU, negative_slope=0.2, inplace=True)))
    layers.append(ConvLayer(256, 512, ks=4, stride=1, norm_type=NormType.Batch, bias=False,
                           act_cls=partial(nn.LeakyReLU, negative_slope=0.2, inplace=True)))
    layers.append(nn.Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1)))
    return nn.Sequential(*layers)

# Cell
class UnetUpsample(Module):
    def __init__(self, ni, nout, hook, ks, padding, dropout=False):
        self.hook = hook
        self.upsample = ConvLayer(ni, nout, ks=ks, stride=2, norm_type=NormType.Batch,
                                  transpose=True, padding=padding, bias=False)
        if dropout:
            layers = list(self.upsample.children())
            layers.append(nn.Dropout(0.5))
            self.upsample = nn.Sequential(*layers)
    def forward(self, x):
        return torch.cat([self.upsample(x), self.hook.stored], dim=1)

class CGenerator(SequentialEx):
    def __init__(self, n_channels, out_channels, enc_l=5):
        encoder = []
        encoder.append(ConvLayer(n_channels, 64, ks=4, stride=2, norm_type=None, bias=False,
                                 act_cls=partial(nn.LeakyReLU, negative_slope=0.2, inplace=True)))
        ni = 64
        for i in range(enc_l):
            nout = min(ni*2, 512)
            encoder.append(ConvLayer(ni, nout, ks=4, stride=2, norm_type=NormType.Batch, bias=False,
                                     act_cls=partial(nn.LeakyReLU, negative_slope=0.2, inplace=True)))
            ni = nout
        nout = min(ni*2, 512)
        encoder.append(ConvLayer(ni, nout, ks=4, stride=2, norm_type=NormType.Batch, bias=False, padding=1))# act_cls=None
        ni = nout
        hooks = hook_outputs(encoder[:-1])
        decoder = []
        for i, (l, h) in enumerate(zip(encoder[-2::-1], hooks[::-1])):
            nout = first(l.children()).out_channels
            ks = 4
            padding = 1
            dropout = i<enc_l-3
            decoder.append(UnetUpsample(ni, nout, h, ks, padding, dropout=dropout))
            ni = 2*nout
        nout = out_channels
        decoder.append(ConvLayer(ni, nout, ks=4, stride=2, norm_type=NormType.Batch, transpose=True, padding=1, bias=True, act_cls=nn.Tanh))
        layers = encoder + decoder
        super().__init__(*layers)
    def forward(self, x):
        return super().forward(x)


# Cell
@patch
def predict(self:GANLearner, item, rm_type_tfms=None, with_input=False):
    dl = self.dls.test_dl([item], rm_type_tfms=rm_type_tfms, num_workers=0)
    inp,preds,_,dec_preds = self.get_preds(dl=dl, with_input=True, with_decoded=True)
    i = getattr(self.dls, 'n_inp', -1)
    inp = (inp,) if i==1 else tuplify(inp)
    n_out = len(self.dls.tls) - i
    dec_preds = (dec_preds,) if n_out==1 else tuplify(dec_preds)
    dec = self.dls.decode_batch(inp + dec_preds)[0]
    dec_inp,dec_targ = map(detuplify, [dec[:i],dec[i:]])
    res = dec_targ,dec_preds[0],preds[0]
    if with_input: res = (dec_inp,) + res
    return res

# Cell
@delegates(GANLearner.__init__)
def pix2pix_learner(dls, gen_arch, critic=None, cut=None, config=None, gen_loss=None, cri_loss=None, switcher=None,
                    opt_func=None, **kwargs):
    "Build a unet learner from `dls` and `arch`"
    b = dls.one_batch()
    size = b[0].shape[-2:]
    critic_n_in = L(b[1]).attrgot('shape').itemgot(1).sum()
    n_out = b[1][-1].shape[1]
    n_in =  L(b[0]).attrgot('shape').itemgot(1).sum() if is_listy(b[0]) else b[0].shape[1]
    if config is None: config = {}
    if not isinstance(gen_arch, ConditionalGenerator):
        if not isinstance(gen_arch, DynamicUnet):
            meta = model_meta.get(gen_arch)
            body = create_body(gen_arch, n_in, False, ifnone(cut, meta['cut']))
            model = DynamicUnet(body, n_out, size, **config)
        else:
            model = gen_arch
        cgen = ConditionalGenerator(model)
    else:
        cgen = gen_arch
    if not isinstance(critic, SiameseCritic):
        critic = SiameseCritic(Patch70(critic_n_in))
    opt_func = ifnone(partial(Adam, mom=0.5, sqr_mom=0.999, wd=0,eps=1e-7), opt_func)
    learn = GANLearner(
                       dls, cgen, critic,
                       ifnone(gen_loss, gen_bce_l1_loss),
                       ifnone(cri_loss, crit_bce_loss),
                       switcher=ifnone(FixedGANSwitcher(n_crit=1, n_gen=1),switcher),
                       opt_func=opt_func,
                       switch_eval=False, **kwargs)

    return learn